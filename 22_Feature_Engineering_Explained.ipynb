{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORtx0htA4eoXPIsWt8EuNg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swopnimghimire-123123/Machine-Learning-Journey/blob/main/22_Feature_Engineering_Explained.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3559f9d5"
      },
      "source": [
        "## Feature Engineering Explained\n",
        "\n",
        "*   **What is feature Engineering?**\n",
        "    Feature engineering is the process of using domain knowledge to create features that make machine learning algorithms work. It is the art of creating new input features for a machine learning algorithm from raw data. This process can significantly improve the performance of machine learning models.\n",
        "\n",
        "*   **Classifying feature Engineering**\n",
        "    Feature engineering can be broadly classified into several categories, including:\n",
        "    *   **Feature Transformation:** Modifying existing features to make them more suitable for modeling (e.g., scaling, log transformation).\n",
        "    *   **Feature Construction:** Creating new features from existing ones (e.g., combining features, polynomial features).\n",
        "    *   **Feature Selection:** Choosing the most relevant features from a dataset.\n",
        "    *   **Feature Extraction:** Transforming features into a lower-dimensional space while preserving important information (e.g., PCA).\n",
        "\n",
        "*   **Missing Values Imputation**\n",
        "    Missing values in a dataset can cause problems for many machine learning algorithms. Imputation is the process of filling in these missing values with estimated values. Common imputation techniques include using the mean, median, mode, or more sophisticated methods like K-Nearest Neighbors (KNN) imputation or regression imputation.\n",
        "\n",
        "*   **Handling Categorical Values**\n",
        "    Machine learning models typically require numerical input, so categorical features (like text labels) need to be converted into a numerical format. Common techniques include:\n",
        "    *   **One-Hot Encoding:** Creating new binary columns for each category.\n",
        "    *   **Label Encoding:** Assigning a unique integer to each category.\n",
        "    *   **Target Encoding:** Encoding categories based on the target variable's mean.\n",
        "\n",
        "*   **Outlier Detection**\n",
        "    Outliers are data points that are significantly different from other observations. They can skew statistical analyses and negatively impact model performance. Outlier detection techniques include using statistical methods (like Z-score or IQR) or machine learning algorithms (like Isolation Forest or Local Outlier Factor).\n",
        "\n",
        "*   **Feature Scaling**\n",
        "    Feature scaling is the process of adjusting the range of features in a dataset. This is important for many machine learning algorithms that are sensitive to the scale of input features, such as gradient descent-based methods and distance-based algorithms (like K-Means or SVM). Common scaling techniques include:\n",
        "    *   **Min-Max Scaling:** Scaling features to a specific range (e.g., 0 to 1).\n",
        "    *   **Standardization:** Scaling features to have a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "*   **Feature Construction**\n",
        "    Feature construction involves creating new features from existing ones to improve the model's ability to capture complex relationships in the data. This can involve combining features, creating interaction terms, extracting information from dates or text, or applying mathematical transformations.\n",
        "\n",
        "*   **Feature Selection**\n",
        "    Feature selection is the process of selecting a subset of relevant features for use in model building. This can help to reduce dimensionality, improve model performance, reduce overfitting, and speed up training. Techniques include filter methods (based on statistical measures), wrapper methods (using a model to evaluate feature subsets), and embedded methods (built into the model training process).\n",
        "\n",
        "*   **Feature Extraction**\n",
        "    Feature extraction is a dimensionality reduction technique that transforms features into a lower-dimensional space while preserving important information. Unlike feature selection, which selects a subset of original features, feature extraction creates new, lower-dimensional features that are combinations of the original features. Common techniques include Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA)."
      ]
    }
  ]
}